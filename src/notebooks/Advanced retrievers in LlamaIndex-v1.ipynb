{"cells":[{"cell_type":"markdown","id":"aef8ee8e-ed1c-4e09-b5e4-3cd98ebd3712","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"ef9c9403-9136-4390-98b4-77e44ab9a4e1","metadata":{},"outputs":[],"source":["# **Explore Advanced Retrievers in LlamaIndex**\n","\n","Estimated time needed: **60** minutes\n","\n","This comprehensive lab demonstrates advanced retrieval techniques in LlamaIndex using IBM watsonx.ai as the foundation model provider. You'll learn core retrievers, advanced retrievers, and sophisticated fusion techniques that power modern RAG applications.\n","\n","Through hands-on examples, you'll master the art of building intelligent information retrieval systems that can handle complex queries, combine multiple search strategies, and deliver precise results for production RAG applications.\n","\n","## __Table of Contents__\n","\n","<ol>\n","    <li><a href=\"#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"#watsonx.ai-LLM-Integration\">watsonx.ai LLM Integration</a></li>\n","            <li><a href=\"#Sample-Data-Setup\">Sample Data Setup</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Background\">Background</a>\n","        <ol>\n","            <li><a href=\"#What-are-Advanced-Retrievers?\">What are Advanced Retrievers?</a></li>\n","            <li><a href=\"#Why-are-Advanced-Retrievers-Important?\">Why are Advanced Retrievers Important?</a></li>\n","            <li><a href=\"#Index-Types-Overview\">Index Types Overview</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Core-Retriever-Demonstrations\">Core Retriever Demonstrations</a>\n","        <ol>\n","            <li><a href=\"#1.-Vector-Index-Retriever---The-Foundation\">Vector Index Retriever - The Foundation</a></li>\n","            <li><a href=\"#2.-BM25-Retriever---Advanced-Keyword-Based-Search\">BM25 Retriever - Advanced Keyword Search</a></li>\n","            <li><a href=\"#3.-Document-Summary-Index-Retrievers\">Document Summary Index Retrievers</a></li>\n","            <li><a href=\"#4.-Auto-Merging-Retriever---Hierarchical-Context-Preservation\">Auto Merging Retriever - Hierarchical Context</a></li>\n","            <li><a href=\"#5.-Recursive-Retriever---Multi-Level-Reference-Following\">Recursive Retriever - Multi-Level Reference Following</a></li>\n","            <li><a href=\"#6.-Query-Fusion-Retriever---Multi-Query-Enhancement-with-Advanced-Fusion\">QueryFusion Retriever - Multi-Query Enhancement</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Exercises\">Exercises</a>\n","        <ol>\n","            <li><a href=\"#Exercise-1---Build-a-Custom-Hybrid-Retriever\">Exercise 1 - Build a Custom Hybrid Retriever</a></li>\n","            <li><a href=\"#Exercise-2---Create-a-Production-RAG-Pipeline\">Exercise 2 - Create a Production RAG Pipeline</a></li>\n","        </ol>\n","    </li>\n","    <li><a href=\"#Summary\">Summary</a></li>\n","    <li><a href=\"#Authors\">Authors</a></li>\n","</ol>\n"]},{"cell_type":"markdown","id":"5c6f8bf7-c123-4019-8422-e5072ae0d305","metadata":{},"outputs":[],"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n","- Understand the different types of retrievers available in LlamaIndex and their use cases\n","- Implement Vector Index Retriever for semantic search\n","- Use BM25 Retriever for keyword-based search with advanced ranking\n","- Create Document Summary Index Retrievers for intelligent document selection\n","- Build Auto Merging Retriever for hierarchical context preservation\n","- Implement Recursive Retriever for multi-level reference following\n","- Master QueryFusion Retriever with advanced fusion techniques (RRF, Relative Score, Distribution-Based)\n","- Compare and contrast different retrieval approaches for various scenarios\n","- Build production-ready RAG pipelines with multiple retrieval strategies\n","\n","---\n"]},{"cell_type":"markdown","id":"9922ed7f-a484-48ee-b9e8-4a10d26d8d1a","metadata":{},"outputs":[],"source":["## Setup\n","\n","For this lab, we will be using the following libraries:\n","\n","*   [`llama-index`](https://docs.llamaindex.ai/) - The core LlamaIndex library for building RAG applications\n","*   [`llama-index-llms-ibm`](https://docs.llamaindex.ai/en/stable/api_reference/llms/ibm/) - IBM watsonx.ai integration for LlamaIndex\n","*   [`llama-index-retrievers-bm25`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/bm25/) - BM25 retriever implementation\n","*   [`llama-index-embeddings-huggingface`](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface/) - HuggingFace embeddings integration\n","*   [`sentence-transformers`](https://www.sbert.net/) - For generating high-quality text embeddings\n","*   [`rank-bm25`](https://github.com/dorianbrown/rank_bm25) - BM25 ranking algorithm implementation\n","*   [`PyStemmer`](https://github.com/snowballstem/pystemmer) - Stemming algorithms for better text processing\n","*   [`ibm-watsonx-ai`](https://ibm.github.io/watsonx-ai-python-sdk/) - IBM watsonx.ai SDK for foundation models\n"]},{"cell_type":"markdown","id":"972315af-5e5b-4a18-a04f-33696a16a84c","metadata":{},"outputs":[],"source":["### Installing Required Libraries\n","\n","Run the following cell to install required libraries.\n","\n","**NOTE**: The installation process takes about **5** minutes to complete. Feel free to grab a coffee in the meantime\n","\n","```\n","  ( (\n","   ) )\n","........\n","|      |]\n","\\      /   \n"," `----'\n","```\n"]},{"cell_type":"code","id":"72425da0-23e3-4ef8-a1b9-c05fe2d963f4","metadata":{},"outputs":[],"source":["!pip install llama-index==0.12.49 \\\n    llama-index-embeddings-huggingface==0.5.5 \\\n    llama-index-llms-ibm==0.4.0 \\\n    llama-index-retrievers-bm25==0.5.2 \\\n    sentence-transformers==5.0.0 \\\n    rank-bm25==0.2.2 \\\n    PyStemmer==2.2.0.3 \\\n    ibm-watsonx-ai==1.3.31 | tail -n 1"]},{"cell_type":"markdown","id":"a5ac0c56-c801-4ef1-b931-c3923c16f3a6","metadata":{},"outputs":[],"source":["### Importing Required Libraries\n","\n","We import all the necessary libraries for this lab, including core LlamaIndex components, retrievers, and IBM watsonx.ai integration:\n"]},{"cell_type":"code","id":"4e923b35-5a45-44bb-b94f-187503f710bb","metadata":{},"outputs":[],"source":["import os\nimport json\nfrom typing import List, Optional\nimport asyncio\nimport warnings\nimport numpy as np\nwarnings.filterwarnings('ignore')\n\n# Core LlamaIndex imports\nfrom llama_index.core import (\n    VectorStoreIndex, \n    SimpleDirectoryReader, \n    Document,\n    Settings,\n    DocumentSummaryIndex,\n    KeywordTableIndex\n)\nfrom llama_index.core.retrievers import (\n    BaseRetriever,\n    VectorIndexRetriever,\n    AutoMergingRetriever,\n    RecursiveRetriever,\n    QueryFusionRetriever\n)\nfrom llama_index.core.indices.document_summary import (\n    DocumentSummaryIndexLLMRetriever,\n    DocumentSummaryIndexEmbeddingRetriever,\n)\nfrom llama_index.core.node_parser import SentenceSplitter, HierarchicalNodeParser\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.postprocessor import SentenceTransformerRerank\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n# Advanced retriever imports\nfrom llama_index.retrievers.bm25 import BM25Retriever\n\n# IBM WatsonX LlamaIndex integration\nfrom ibm_watsonx_ai import APIClient\nfrom llama_index.llms.ibm import WatsonxLLM\n\n# Sentence transformers\nfrom sentence_transformers import SentenceTransformer\n\n# Statistical libraries for fusion techniques\ntry:\n    from scipy import stats\n    SCIPY_AVAILABLE = True\nexcept ImportError:\n    SCIPY_AVAILABLE = False\n    print(\"⚠️ scipy not available - some advanced fusion features will be limited\")\n\nprint(\"✅ All imports successful!\")"]},{"cell_type":"markdown","id":"3049a1e2-7449-42a3-ae15-61ec6397d658","metadata":{},"outputs":[],"source":["## watsonx.ai LLM Integration\n","\n","We'll create custom wrapper classes to integrate IBM watsonx.ai with LlamaIndex. This allows us to use watsonx.ai foundation models while maintaining compatibility with all LlamaIndex retrievers.\n"]},{"cell_type":"code","id":"29ed940d-7ff2-437a-8cb2-520c0f0a1fdb","metadata":{},"outputs":[],"source":["# watsonx.ai LLM using official LlamaIndex integration\ndef create_watsonx_llm():\n    \"\"\"Create watsonx.ai LLM instance using official LlamaIndex integration.\"\"\"\n    try:\n        # Create the API client object\n        api_client = APIClient({'url': \"https://us-south.ml.cloud.ibm.com\"})\n        # Use llama-index-llms-ibm (official watsonx.ai integration)\n        llm = WatsonxLLM(\n            model_id=\"ibm/granite-3-3-8b-instruct\",\n            url=\"https://us-south.ml.cloud.ibm.com\",\n            project_id=\"skills-network\",\n            api_client=api_client,\n            temperature=0.9\n        )\n        print(\"✅ watsonx.ai LLM initialized using official LlamaIndex integration\")\n        return llm\n    except Exception as e:\n        print(f\"⚠️ watsonx.ai initialization error: {e}\")\n        print(\"Falling back to mock LLM for demonstration\")\n        \n        # Fallback mock LLM for demonstration\n        from llama_index.core.llms.mock import MockLLM\n        return MockLLM(max_tokens=512)"]},{"cell_type":"code","id":"c7fc63fe-56e0-4017-bb2e-daa10cfe3c64","metadata":{},"outputs":[],"source":["# Initialize embedding model first\nprint(\"🔧 Initializing HuggingFace embeddings...\")\nembed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\nprint(\"✅ HuggingFace embeddings initialized!\")\n\n# Setup with watsonx.ai\nprint(\"🔧 Initializing watsonx.ai LLM...\")\nllm = create_watsonx_llm()\n\n# Configure global settings\nSettings.llm = llm\nSettings.embed_model = embed_model\nprint(\"✅ watsonx.ai LLM and embeddings configured!\")"]},{"cell_type":"markdown","id":"28bd7269-4601-4d91-8604-ac04b83072d4","metadata":{},"outputs":[],"source":["---\n","\n","## Background\n","\n","Before diving into the advanced retrieval techniques, let's understand the foundational concepts that make these retrievers powerful.\n","\n","### What are Advanced Retrievers?\n","\n","Advanced retrievers in LlamaIndex are sophisticated components that go beyond simple vector similarity search to provide more nuanced, context-aware, and intelligent information retrieval. They combine multiple techniques such as:\n","\n","- **Semantic Understanding**: Using embeddings to understand meaning and context\n","- **Keyword Matching**: Precise term-based search for exact specifications\n","- **Hierarchical Context**: Maintaining relationships between different levels of information\n","- **Multi-Query Processing**: Generating and combining results from multiple query variations\n","- **Fusion Techniques**: Intelligently combining results from different retrieval methods\n","\n","### Why are Advanced Retrievers Important?\n","\n","1. **Improved Accuracy**: Advanced retrievers can find more relevant information by using multiple search strategies\n","2. **Better Context Preservation**: They maintain important relationships between pieces of information\n","3. **Reduced Hallucination**: More precise retrieval leads to more accurate AI responses\n","4. **Scalability**: Efficient retrieval strategies work better with large document collections\n","5. **Flexibility**: Different retrieval methods can be combined for optimal results\n","\n","### Index Types Overview\n","\n","Before exploring advanced retrievers, it's helpful to first understand the three main index types supported by LlamaIndex. Each is designed to support different retrieval scenarios:\n","\n","**VectorStoreIndex:**\n","- Stores vector embeddings for each document chunk\n","- Best suited for semantic retrieval based on meaning\n","- Commonly used in LLM pipelines and RAG applications\n","\n","**DocumentSummaryIndex:**\n","- Generates and stores summaries of documents at indexing time\n","- Uses summaries to filter documents before retrieving full content\n","- Especially useful for large and diverse document sets that cannot fit in the context window of an LLM\n","\n","**KeywordTableIndex:**\n","- Extracts keywords from documents and maps them to specific content chunks\n","- Enables exact keyword matching for rule-based or hybrid search scenarios\n","- Ideal for applications requiring precise term matching\n","\n","## Sample Data Setup\n","\n","We'll use a collection of AI and machine learning documents to demonstrate different retrieval strategies.\n"]},{"cell_type":"code","id":"d6dd09a1-f2d1-4948-911b-6bc756965895","metadata":{},"outputs":[],"source":["# Sample data for the lab - AI/ML focused documents\nSAMPLE_DOCUMENTS = [\n    \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n    \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n    \"Natural language processing enables computers to understand, interpret, and generate human language.\",\n    \"Computer vision allows machines to interpret and understand visual information from the world.\",\n    \"Reinforcement learning is a type of machine learning where agents learn to make decisions through rewards and penalties.\",\n    \"Supervised learning uses labeled training data to learn a mapping from inputs to outputs.\",\n    \"Unsupervised learning finds hidden patterns in data without labeled examples.\",\n    \"Transfer learning leverages knowledge from pre-trained models to improve performance on new tasks.\",\n    \"Generative AI can create new content including text, images, code, and more.\",\n    \"Large language models are trained on vast amounts of text data to understand and generate human-like text.\"\n]\n\n# Consistent query examples used throughout the lab\nDEMO_QUERIES = {\n    \"basic\": \"What is machine learning?\",\n    \"technical\": \"neural networks deep learning\", \n    \"learning_types\": \"different types of learning\",\n    \"advanced\": \"How do neural networks work in deep learning?\",\n    \"applications\": \"What are the applications of AI?\",\n    \"comprehensive\": \"What are the main approaches to machine learning?\",\n    \"specific\": \"supervised learning techniques\"\n}\n\nprint(f\"📄 Loaded {len(SAMPLE_DOCUMENTS)} sample documents\")\nprint(f\"🔍 Prepared {len(DEMO_QUERIES)} consistent demo queries\")\nfor i, doc in enumerate(SAMPLE_DOCUMENTS[:3], 1):\n    print(f\"{i}. {doc}\")\nprint(\"...\")"]},{"cell_type":"markdown","id":"d23d4544-2561-46f5-ba18-721bd72d5f9a","metadata":{},"outputs":[],"source":["## Initialize Lab Environment\n","\n","Let's create our lab class and initialize all the indexes we'll need for different retrievers.\n"]},{"cell_type":"code","id":"29c18cf6-59b0-4557-9a7c-4ccc0916b1d3","metadata":{},"outputs":[],"source":["class AdvancedRetrieversLab:\n    def __init__(self):\n        print(\"🚀 Initializing Advanced Retrievers Lab...\")\n        self.documents = [Document(text=text) for text in SAMPLE_DOCUMENTS]\n        self.nodes = SentenceSplitter().get_nodes_from_documents(self.documents)\n        \n        print(\"📊 Creating indexes...\")\n        # Create various indexes\n        self.vector_index = VectorStoreIndex.from_documents(self.documents)\n        self.document_summary_index = DocumentSummaryIndex.from_documents(self.documents)\n        self.keyword_index = KeywordTableIndex.from_documents(self.documents)\n        \n        print(\"✅ Advanced Retrievers Lab Initialized!\")\n        print(f\"📄 Loaded {len(self.documents)} documents\")\n        print(f\"🔢 Created {len(self.nodes)} nodes\")\n\n# Initialize the lab\nlab = AdvancedRetrieversLab()"]},{"cell_type":"markdown","id":"5147c545-095d-4e3e-a21c-391d16154e68","metadata":{},"outputs":[],"source":["---\n","\n","# Core Retriever Demonstrations\n","\n","This lab focuses on the essential retrievers in LlamaIndex, covering core retrieval methods, advanced retrievers, and fusion techniques. Each section provides practical examples and detailed explanations based on official LlamaIndex documentation.\n"]},{"cell_type":"markdown","id":"95924dd6-60be-4c06-bce0-06095ec56334","metadata":{},"outputs":[],"source":["## 1. Vector Index Retriever - The Foundation\n","\n","The Vector Index Retriever uses vector embeddings to find semantically related content, making it ideal for general-purpose search and widely used in retrieval-augmented generation (RAG) pipelines.\n","\n","**How it works**: \n","- Documents are split into nodes and embedded using the configured embedding model\n","- Query is converted to an embedding vector\n","- Returns nodes ranked by cosine similarity to the query embedding\n","- Generates embeddings in batches of 2048 nodes by default\n","\n","**When to use:**\n","- General-purpose semantic search (most common use case)\n","- Finding conceptually related content based on meaning rather than exact keywords\n","- RAG pipelines where semantic understanding is crucial\n","- When exact keyword matching isn't the primary requirement\n","\n","**Key characteristics from authoritative source:**\n","- **Stores embeddings for each document chunk** (VectorStoreIndex foundation)\n","- **Best for semantic retrieval** based on meaning and context\n","- **Commonly used in LLM pipelines** for retrieval-augmented generation\n","\n","**Strengths**: \n","- Excellent semantic understanding and context awareness\n","- Handles synonyms and related concepts effectively\n","- Works well with natural language queries\n","\n","**Limitations**: \n","- May miss exact keyword matches when specific terms are crucial\n","- Requires a good embedding model for optimal performance\n","- Can be computationally intensive for large document collections\n"]},{"cell_type":"code","id":"1902624c-685e-4085-bb1a-91c3c1dcd528","metadata":{},"outputs":[],"source":["print(\"=\" * 60)\nprint(\"1. VECTOR INDEX RETRIEVER\")\nprint(\"=\" * 60)\n\n# Basic vector retriever\nvector_retriever = VectorIndexRetriever(\n    index=lab.vector_index,\n    similarity_top_k=3\n)\n\n# Alternative creation method\nalt_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n\nquery = DEMO_QUERIES[\"basic\"]  # \"What is machine learning?\"\nnodes = vector_retriever.retrieve(query)\n\nprint(f\"Query: {query}\")\nprint(f\"Retrieved {len(nodes)} nodes:\")\nfor i, node in enumerate(nodes, 1):\n    print(f\"{i}. Score: {node.score:.4f}\")\n    print(f\"   Text: {node.text[:100]}...\")\n    print()"]},{"cell_type":"markdown","id":"769c6b00-2098-450e-9b63-f9a5f97129f0","metadata":{},"outputs":[],"source":["## 2. BM25 Retriever - Advanced Keyword-Based Search\n","\n","BM25 is a keyword-based retrieval method that improves on TF-IDF by addressing some of its key limitations. It's widely used in production search systems including Elasticsearch and Apache Lucene.\n","\n","### Understanding TF-IDF: The Foundation\n","\n","Before diving into BM25, let's understand **TF-IDF** (Term Frequency-Inverse Document Frequency), which BM25 builds upon:\n","\n","**Term Frequency (TF)**: Measures how often a word appears in a document\n","- Example: If \"neural\" appears 3 times in a 100-word document, TF = 3/100 = 0.03\n","\n","**Inverse Document Frequency (IDF)**: Measures how rare a word is across all documents\n","- Example: If \"neural\" appears in only 2 out of 1000 documents, IDF = log(1000/2) = 6.21\n","- Common words like \"the\" have low IDF; rare technical terms have high IDF\n","\n","**TF-IDF Score**: TF × IDF\n","- Highlights words that are frequent in one document but rare across the collection\n","- Developed by Karen Spärck Jones, who pioneered the concept of term specificity\n","\n","### How BM25 Improves Upon TF-IDF\n","\n","**Key BM25 Improvements:**\n","\n","1. **Term Frequency Saturation**: BM25 reduces the impact of repeated terms using term frequency saturation\n","   - Problem: In TF-IDF, if a word appears 100 times vs 10 times, the score increases linearly\n","   - Solution: BM25 uses a saturation function that plateaus after a certain frequency\n","\n","2. **Document Length Normalization**: BM25 adjusts for document length, making it more effective for keyword-based search\n","   - Problem: In TF-IDF, longer documents have unfair advantages\n","   - Solution: BM25 normalizes scores based on document length relative to average\n","\n","3. **Tunable Parameters**: Allows fine-tuning for different types of content\n","   - k1 ≈ 1.2: Controls term frequency saturation (how quickly scores plateau)\n","   - b ≈ 0.75: Controls document length normalization (0=none, 1=full)\n","\n","### When to Use BM25\n","\n","**Ideal for:**\n","- Technical documentation where exact terms matter\n","- Legal documents with specific terminology\n","- Product catalogs with precise specifications\n","- Academic papers with specialized vocabulary\n","- Applications requiring keyword-based retrieval rather than semantic similarity\n","\n","**Advantages:**\n","- Excellent precision for exact term matches\n","- Fast computational performance\n","- Proven effectiveness in production systems\n","- No training required (unlike neural approaches)\n","- Interpretable scoring mechanism\n","\n","**Limitations:**\n","- No semantic understanding (doesn't handle synonyms)\n","- Struggles with typos and variations\n","- Limited context understanding\n","- Requires careful parameter tuning for optimal performance\n"]},{"cell_type":"code","id":"21d36098-fe37-436b-adb3-8b1b608fa9c7","metadata":{},"outputs":[],"source":["print(\"=\" * 60)\nprint(\"2. BM25 RETRIEVER\")\nprint(\"=\" * 60)\n\ntry:\n    import Stemmer\n    \n    # Create BM25 retriever with default parameters\n    bm25_retriever = BM25Retriever.from_defaults(\n        nodes=lab.nodes,\n        similarity_top_k=3,\n        stemmer=Stemmer.Stemmer(\"english\"),\n        language=\"english\"\n    )\n    \n    query = DEMO_QUERIES[\"technical\"]  # \"neural networks deep learning\"\n    nodes = bm25_retriever.retrieve(query)\n    \n    print(f\"Query: {query}\")\n    print(\"BM25 analyzes exact keyword matches with sophisticated scoring\")\n    print(f\"Retrieved {len(nodes)} nodes:\")\n    \n    for i, node in enumerate(nodes, 1):\n        score = node.score if hasattr(node, 'score') and node.score else 0\n        print(f\"{i}. BM25 Score: {score:.4f}\")\n        print(f\"   Text: {node.text[:100]}...\")\n        \n        # Highlight which query terms appear in the text\n        text_lower = node.text.lower()\n        query_terms = query.lower().split()\n        found_terms = [term for term in query_terms if term in text_lower]\n        if found_terms:\n            print(f\"   → Found terms: {found_terms}\")\n        print()\n    \n    print(\"BM25 vs TF-IDF Comparison:\")\n    print(\"TF-IDF Problem: Linear term frequency scaling\")\n    print(\"  Example: 10 occurrences → score of 10, 100 occurrences → score of 100\")\n    print(\"BM25 Solution: Saturation function\")\n    print(\"  Example: 10 occurrences → high score, 100 occurrences → slightly higher score\")\n    print()\n    print(\"TF-IDF Problem: No document length consideration\")\n    print(\"  Example: Long documents dominate results\")\n    print(\"BM25 Solution: Length normalization (b parameter)\")\n    print(\"  Example: Scores adjusted based on document length vs. average\")\n    print()\n    print(\"Key BM25 Parameters:\")\n    print(\"- k1 ≈ 1.2: Term frequency saturation (how quickly scores plateau)\")\n    print(\"- b ≈ 0.75: Document length normalization (0=none, 1=full)\")\n    print(\"- IDF weighting: Rare terms get higher scores\")\n        \nexcept ImportError:\n    print(\"⚠️ BM25Retriever requires 'pip install PyStemmer'\")\n    print(\"Demonstrating BM25 concepts with fallback vector search...\")\n    \n    fallback_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n    query = DEMO_QUERIES[\"technical\"]\n    nodes = fallback_retriever.retrieve(query)\n    \n    print(f\"Query: {query}\")\n    print(\"(Using vector fallback to demonstrate BM25 concepts)\")\n    \n    for i, node in enumerate(nodes, 1):\n        print(f\"{i}. Vector Score: {node.score:.4f}\")\n        print(f\"   Text: {node.text[:100]}...\")\n        \n        # Demonstrate TF-IDF concept manually\n        text_lower = node.text.lower()\n        query_terms = query.lower().split()\n        found_terms = [term for term in query_terms if term in text_lower]\n        \n        if found_terms:\n            print(f\"   → BM25 would boost this result for terms: {found_terms}\")\n        print()\n    \n    print(\"BM25 Concept Demonstration:\")\n    print(\"1. TF-IDF Foundation:\")\n    print(\"   - Term Frequency: How often words appear in document\")\n    print(\"   - Inverse Document Frequency: How rare words are across collection\")\n    print(\"   - TF-IDF = TF × IDF (balances frequency vs rarity)\")\n    print()\n    print(\"2. BM25 Improvements:\")\n    print(\"   - Saturation: Prevents over-scoring repeated terms\")\n    print(\"   - Length normalization: Prevents long document bias\")\n    print(\"   - Tunable parameters: k1 (saturation) and b (length adjustment)\")\n    print()\n    print(\"3. Real-world Usage:\")\n    print(\"   - Elasticsearch default scoring function\")\n    print(\"   - Apache Lucene/Solr standard\")\n    print(\"   - Used in 83% of text-based recommender systems\")\n    print(\"   - Developed by Robertson & Spärck Jones at City University London\")"]},{"cell_type":"markdown","id":"64251976-c241-4224-9ae9-eaa1f24e7655","metadata":{},"outputs":[],"source":["## 3. Document Summary Index Retrievers\n","\n","Document Summary Index Retrievers use document summaries instead of the actual documents to find relevant content, making them efficient for large collections. **They return the original documents, not their summaries.**\n","\n","**How it works (from authoritative source)**:\n","- **Generates and stores summaries of documents** at indexing time\n","- **Uses summaries to filter documents** before retrieving full content\n","- **Two-stage Process**: First uses summaries to filter documents, then returns full document content\n","- **Especially useful for large, diverse corpora** that cannot fit in the context window of an LLM\n","\n","**Two Retrieval Options**: \n","1. **DocumentSummaryIndexLLMRetriever**: \n","   - Uses a large language model to analyze the query against document summaries\n","   - Provides intelligent document selection but can be more time-consuming and expensive\n","   - Best for complex queries requiring nuanced understanding\n","\n","2. **DocumentSummaryIndexEmbeddingRetriever**: \n","   - Uses semantic similarity between the query and summary embeddings\n","   - Faster and more cost-effective than LLM-based approach\n","   - Good for straightforward similarity matching\n","\n","**When to use (based on authoritative guidance):**\n","- Large document collections where documents cover different topics\n","- When you need efficient document-level filtering before detailed retrieval\n","- Multi-document QA where documents have distinct subject matters\n","- Large and diverse document sets that cannot fit in the context window of an LLM\n","\n","**Configuration Parameters:**\n","- `choice_top_k` (LLM retriever): Number of documents to select\n","- `similarity_top_k` (Embedding retriever): Number of documents to select\n","- Default is 1, increase for multiple document retrieval\n","\n","**Key Point**: **Returns original documents, not their summaries** - the summaries are only used for filtering\n","\n","**Strengths**: \n","- Efficient document selection and reduces search space\n","- Good for heterogeneous collections with diverse topics\n","- Returns original documents with full context intact\n","\n","**Limitations**: \n","- Requires LLM for summary generation during indexing\n","- May lose some detail present in original documents during summary creation\n","- LLM-based version can be slower and more expensive than other options\n"]},{"cell_type":"code","id":"314066bf-3c31-4188-a747-ed6607fb1bf9","metadata":{},"outputs":[],"source":["print(\"=\" * 60)\nprint(\"3. DOCUMENT SUMMARY INDEX RETRIEVERS\")\nprint(\"=\" * 60)\n\n# LLM-based document summary retriever\ndoc_summary_retriever_llm = DocumentSummaryIndexLLMRetriever(\n    lab.document_summary_index,\n    choice_top_k=3  # Number of documents to select\n)\n\n# Embedding-based document summary retriever  \ndoc_summary_retriever_embedding = DocumentSummaryIndexEmbeddingRetriever(\n    lab.document_summary_index,\n    similarity_top_k=3  # Number of documents to select\n)\n\nquery = DEMO_QUERIES[\"learning_types\"]  # \"different types of learning\"\n\nprint(f\"Query: {query}\")\n\nprint(\"\\nA) LLM-based Document Summary Retriever:\")\nprint(\"Uses LLM to select relevant documents based on summaries\")\ntry:\n    nodes_llm = doc_summary_retriever_llm.retrieve(query)\n    print(f\"Retrieved {len(nodes_llm)} nodes\")\n    for i, node in enumerate(nodes_llm[:2], 1):\n        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Document summary)\")\n        print(f\"   Text: {node.text[:80]}...\")\n        print()\nexcept Exception as e:\n    print(f\"LLM-based retrieval demo: {str(e)[:100]}...\")\n\nprint(\"B) Embedding-based Document Summary Retriever:\")\nprint(\"Uses vector similarity between query and document summaries\")\ntry:\n    nodes_emb = doc_summary_retriever_embedding.retrieve(query)\n    print(f\"Retrieved {len(nodes_emb)} nodes\")\n    for i, node in enumerate(nodes_emb[:2], 1):\n        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Document summary)\")\n        print(f\"   Text: {node.text[:80]}...\")\n        print()\nexcept Exception as e:\n    print(f\"Embedding-based retrieval demo: {str(e)[:100]}...\")\n\nprint(\"Document Summary Index workflow:\")\nprint(\"1. Generates summaries for each document using LLM\")\nprint(\"2. Uses summaries to select relevant documents\")\nprint(\"3. Returns full content from selected documents\")"]},{"cell_type":"markdown","id":"ff964748-739a-4d8d-8e7d-9a8fd3ab9beb","metadata":{},"outputs":[],"source":["## 4. Auto Merging Retriever - Hierarchical Context Preservation\n","\n","Auto Merging Retriever is designed to preserve context in long documents using a hierarchical structure. **It uses hierarchical chunking to break documents into parent and child nodes, and if enough child nodes from the same parent are retrieved, the retriever returns the parent node instead.**\n","\n","**How it works (from authoritative source)**:\n","- **Uses hierarchical chunking** to break documents into parent and child nodes\n","- **Retrieves parent if enough children match** - intelligent merging logic\n","- **Preserves context in long documents** by consolidating related content\n","- **Dual Storage**: Smaller child chunks are indexed in the vector store for precise matching, while larger parent chunks are stored in the docstore\n","\n","**Key behavior pattern**:\n","- Child chunks enable precise matching for specific queries\n","- When multiple child chunks from the same parent are retrieved, the system returns the parent chunk\n","- This **helps consolidate related content and preserve broader context**\n","\n","**When to use (based on authoritative guidance):**\n","- Long documents where small chunks lose important surrounding context\n","- Legal documents, research papers, technical specifications that need context preservation\n","- When you need both precise matching and comprehensive context\n","- Documents with natural hierarchical structure (sections, subsections)\n","\n","**Configuration:**\n","- `chunk_sizes`: List of chunk sizes from largest to smallest (e.g., [512, 256, 128])\n","- `chunk_overlap`: Overlap between chunks to maintain continuity\n","- Storage context manages both vector store (child nodes) and docstore (parent nodes)\n","\n","**Strengths**: \n","- Automatically preserves context without manual intervention\n","- Reduces information fragmentation in long documents\n","- Intelligent merging based on retrieval patterns\n","- Maintains granular search capability while providing broader context\n","\n","**Limitations**: \n","- More complex setup compared to basic retrievers\n","- Requires hierarchical document structure to be effective\n","- Higher storage overhead due to multiple chunk levels\n","- May not be suitable for very short documents\n","\n","*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever/*\n"]},{"cell_type":"code","id":"d718cd9c-db90-4816-a872-70e48082a577","metadata":{},"outputs":[],"source":["print(\"=\" * 60)\nprint(\"4. AUTO MERGING RETRIEVER\")\nprint(\"=\" * 60)\n\n# Create hierarchical nodes\nnode_parser = HierarchicalNodeParser.from_defaults(\n    chunk_sizes=[512, 256, 128]\n)\n\nhier_nodes = node_parser.get_nodes_from_documents(lab.documents)\n\n# Create storage context with all nodes\nfrom llama_index.core import StorageContext\nfrom llama_index.core.storage.docstore import SimpleDocumentStore\nfrom llama_index.core.vector_stores import SimpleVectorStore\n\ndocstore = SimpleDocumentStore()\ndocstore.add_documents(hier_nodes)\n\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\n# Create base index\nbase_index = VectorStoreIndex(hier_nodes, storage_context=storage_context)\nbase_retriever = base_index.as_retriever(similarity_top_k=6)\n\n# Create auto-merging retriever\nauto_merging_retriever = AutoMergingRetriever(\n    base_retriever, \n    storage_context,\n    verbose=True\n)\n\nquery = DEMO_QUERIES[\"advanced\"]  # \"How do neural networks work in deep learning?\"\nnodes = auto_merging_retriever.retrieve(query)\n\nprint(f\"Query: {query}\")\nprint(f\"Auto-merged to {len(nodes)} nodes\")\nfor i, node in enumerate(nodes[:3], 1):\n    print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Auto-merged)\")\n    print(f\"   Text: {node.text[:120]}...\")\n    print()"]},{"cell_type":"markdown","id":"ffdbc028-6375-4586-a63a-621bf64b8d3a","metadata":{},"outputs":[],"source":["## 5. Recursive Retriever - Multi-Level Reference Following\n","\n","The Recursive Retriever is **designed to follow relationships between nodes using references**. **It can follow references from one node to another, such as citations in academic papers or other metadata links**, allowing it to **retrieve related content across documents or layers of abstraction**.\n","\n","**How it works (from authoritative source)**:\n","- **Follows node references** - traverses relationships to find referenced content\n","- **Supports chunk and metadata linking** - handles different types of references\n","- **Multi-Level Navigation**: Can execute sub-queries on referenced retrievers or query engines\n","- **Network Building**: Creates a network of interconnected retrievers that can reference each other\n","\n","**Reference Types Supported**:\n","1. **Chunk References**: Smaller child chunks refer to larger parent chunks for additional context\n","2. **Metadata References**: Summaries or generated questions refer to larger content chunks, such as citations in academic papers\n","\n","**When to use (based on authoritative guidance):**\n","- **Academic papers with citations** and extensive references\n","- **Research papers** where you need to retrieve relevant content from cited papers\n","- Documentation with cross-references and linked content\n","- Knowledge bases with interconnected information\n","- When nodes reference structured data (tables, databases, other documents)\n","\n","**Configuration:**\n","- `retriever_dict`: Maps node IDs or keys to specific retrievers\n","- `query_engine_dict`: Maps keys to query engines for sub-queries\n","- Node metadata can contain references to other nodes or data structures\n","\n","**Key capability**: **Retrieves related content across documents** by following reference chains\n","\n","**Strengths**: \n","- Follows complex relationships and enables multi-step reasoning\n","- Provides comprehensive coverage across related documents\n","- Excellent for handling interconnected information systems\n","- Can traverse multiple levels of references automatically\n","\n","**Limitations**: \n","- Requires careful setup of node relationships\n","- Can be computationally expensive for deep reference chains\n","- Complex debugging when reference chains are extensive\n","- May retrieve too much related content if not properly configured\n","\n","*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/recurisve_retriever_nodes_braintrust/*\n"]},{"cell_type":"code","id":"400333fc-020d-4b8b-a869-62280a249a3b","metadata":{},"outputs":[],"source":["print(\"=\" * 60)\nprint(\"5. RECURSIVE RETRIEVER\")\nprint(\"=\" * 60)\n\n# Create documents with references\ndocs_with_refs = []\nfor i, doc in enumerate(lab.documents):\n    # Add reference metadata\n    ref_doc = Document(\n        text=doc.text,\n        metadata={\n            \"doc_id\": f\"doc_{i}\",\n            \"references\": [f\"doc_{j}\" for j in range(len(lab.documents)) if j != i][:2]\n        }\n    )\n    docs_with_refs.append(ref_doc)\n\n# Create index with referenced documents\nref_index = VectorStoreIndex.from_documents(docs_with_refs)\n\n# Create retriever mapping\nretriever_dict = {\n    f\"doc_{i}\": ref_index.as_retriever(similarity_top_k=1)\n    for i in range(len(docs_with_refs))\n}\n\n# Base retriever\nbase_retriever = ref_index.as_retriever(similarity_top_k=2)\n\n# Add the root retriever to the dictionary\nretriever_dict[\"vector\"] = base_retriever\n\n# Recursive retriever\nrecursive_retriever = RecursiveRetriever(\n    \"vector\",\n    retriever_dict=retriever_dict,\n    query_engine_dict={},\n    verbose=True\n)\n\nquery = DEMO_QUERIES[\"applications\"]  # \"What are the applications of AI?\"\ntry:\n    nodes = recursive_retriever.retrieve(query)\n    print(f\"Query: {query}\")\n    print(f\"Recursively retrieved {len(nodes)} nodes\")\n    for i, node in enumerate(nodes[:3], 1):\n        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Recursive)\")\n        print(f\"   Text: {node.text[:100]}...\")\n        print()\nexcept Exception as e:\n    print(f\"Query: {query}\")\n    print(f\"Recursive retriever demo: {str(e)}\")\n    print(\"Note: Recursive retriever requires specific node reference setup\")\n    \n    # Fallback to basic retrieval for demonstration\n    print(\"\\nFalling back to basic retrieval demonstration...\")\n    base_nodes = base_retriever.retrieve(query)\n    for i, node in enumerate(base_nodes[:2], 1):\n        print(f\"{i}. Score: {node.score:.4f}\")\n        print(f\"   Text: {node.text[:100]}...\")\n        print()"]},{"cell_type":"markdown","id":"9180896e-3112-477f-8f47-5a6f52ab17f8","metadata":{},"outputs":[],"source":["## 6. Query Fusion Retriever - Multi-Query Enhancement with Advanced Fusion\n","\n","The Query Fusion Retriever **combines results from different retrievers** (such as vector-based and keyword-based methods) and **optionally generates multiple variations of a query using an LLM to improve coverage**. **The results are merged using fusion strategies** to improve recall.\n","\n","**How it works (from authoritative source)**:\n","- **Combines results from multiple retrievers** - e.g., vector-based and keyword-based methods\n","- **Supports multiple query variations** - generates different formulations of the same query\n","- **Uses fusion strategies to improve recall** - sophisticated merging techniques\n","- **Improved Coverage**: Reduces impact of query formulation on final results\n","\n","**Core capabilities**:\n","1. **Multiple Retriever Support**: Combines results from different retrievers\n","2. **Query Variation Generation**: Optionally generates multiple variations of a query using an LLM\n","3. **Fusion Strategies**: Merges results using sophisticated fusion techniques\n","\n","**Fusion Strategies Supported (from authoritative source)**:\n","1. **Reciprocal Rank Fusion (RRF)**: **Combines rankings across queries** - robust and doesn't rely on score magnitudes\n","2. **Relative Score Fusion**: **Normalizes scores within each result set** - preserves the relative confidence of each retriever\n","3. **Distribution-Based Fusion**: **Uses statistical normalization** - ideal for handling score variability\n","\n","**When to use (based on authoritative guidance):**\n","- General Q&A where you want to combine semantic relevance with keyword matching\n","- Complex or ambiguous queries that may benefit from multiple formulations\n","- When query phrasing significantly impacts results\n","- Research and exploratory search scenarios\n","- When users provide under-specified or unclear queries\n","\n","**Configuration:**\n","- `num_queries`: Number of query variations to generate (default: 4)\n","- `mode`: Fusion strategy (\"reciprocal_rerank\", \"relative_score\", \"dist_based_score\")\n","- `similarity_top_k`: Number of results to retrieve per query\n","- `use_async`: Enable async processing for better performance\n","\n","**Key benefit**: **Uses fusion strategies such as reciprocal rank fusion or relative score fusion** to intelligently combine results\n","\n","**Strengths**: \n","- Improved recall through multiple query formulations\n","- Handles query variations effectively\n","- Reduces query sensitivity\n","- Combines strengths of different retrieval methods\n","\n","**Limitations**: \n","- Higher computational cost due to multiple retrievers/queries\n","- Requires LLM for query generation (additional cost)\n","- May introduce noise if fusion strategies are not well-tuned\n","- More complex setup and configuration\n"]},{"cell_type":"code","id":"c3694372-d230-4b5f-84fd-5267247f03c5","metadata":{},"outputs":[],"source":["print(\"=\" * 60)\nprint(\"6. QUERY FUSION RETRIEVER - OVERVIEW\")\nprint(\"=\" * 60)\n\n# Create base retriever\nbase_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n\nquery = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\nprint(f\"Query: {query}\")\nprint(\"QueryFusionRetriever generates multiple query variations and fuses results\")\nprint(\"using one of three sophisticated fusion modes.\")\n\nprint(\"\\nOverview of Fusion Modes:\")\nprint(\"1. RECIPROCAL_RERANK: Uses reciprocal rank fusion (most robust)\")\nprint(\"2. RELATIVE_SCORE: Preserves score magnitudes (most interpretable)\")  \nprint(\"3. DIST_BASED_SCORE: Statistical normalization (most sophisticated)\")\n\nprint(\"\\nDemonstration workflow:\")\nprint(\"Each subsection below explores one fusion mode in detail with:\")\nprint(\"- Theoretical explanation of the fusion method\")\nprint(\"- Live demonstration using QueryFusionRetriever\")\nprint(\"- Manual implementation showing the underlying mathematics\")\nprint(\"- Use case recommendations and trade-offs\")\n\nprint(f\"\\nUsing consistent test query throughout: '{query}'\")\nprint(\"This allows direct comparison of how each fusion mode handles the same input.\")\n\nprint(\"\\nProceed to subsections 6.1, 6.2, and 6.3 for detailed demonstrations...\")"]},{"cell_type":"markdown","id":"824247da-ed08-4e48-9f82-91259abbb9d9","metadata":{},"outputs":[],"source":["### 6.1 Reciprocal Rank Fusion (RRF) Mode\n","\n","Reciprocal Rank Fusion is the most robust fusion method in QueryFusionRetriever, designed to combine ranked lists from multiple query variations by using the reciprocal of ranks, which reduces the impact of outliers and provides stable fusion results.\n","\n","**How it works within QueryFusionRetriever**:\n","- Generates multiple query variations (e.g., \"machine learning approaches\", \"ML techniques\", \"learning algorithms\")\n","- Retrieves results for each query variation\n","- Calculates reciprocal rank score: `1 / (rank + k)` where k is typically 60\n","- Sums reciprocal rank scores across all query variations for each document\n","- Re-ranks documents by combined RRF scores\n","\n","**Mathematical formula**:\n","```\n","RRF_score(d) = Σ (1 / (rank_i(d) + k))\n","```\n","Where:\n","- `d` is a document\n","- `rank_i(d)` is the rank of document d in query variation i's results\n","- `k` is a constant (typically 60) that controls the fusion behavior\n","\n","**Why RRF works well for query fusion**:\n","- **Scale-invariant**: Works regardless of individual query result score ranges\n","- **Robust to outliers**: Reciprocal function reduces impact of extreme rankings\n","- **Query-agnostic**: Doesn't depend on specific query formulations\n","- **Proven effectiveness**: Well-established in information retrieval research\n","\n","**When to use RRF mode**:\n","- Default choice for most query fusion scenarios\n","- When query variations might have very different result qualities\n","- When you want stable, predictable fusion behavior\n","- For production systems requiring consistent performance\n","\n","**Advantages**:\n","- Most stable fusion method across different query types\n","- No parameter tuning required beyond the standard k=60\n","- Handles varying numbers of results per query variation gracefully\n","- Computationally efficient\n","\n","**Limitations**:\n","- Loses absolute score information from individual queries\n","- Treats all query variations equally (no weighting)\n","- May not leverage score magnitude differences effectively\n","\n","*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion/*\n"]},{"cell_type":"code","id":"5c74e0c6-ff56-480d-9a4a-ff3c8e630a25","metadata":{},"outputs":[],"source":["print(\"=\" * 60)\nprint(\"6.1 RECIPROCAL RANK FUSION MODE DEMONSTRATION\")\nprint(\"=\" * 60)\n\n# Create QueryFusionRetriever with RRF mode\nbase_retriever = lab.vector_index.as_retriever(similarity_top_k=5)\n\nprint(\"Testing QueryFusionRetriever with reciprocal_rerank mode:\")\nprint(\"This demonstrates how RRF works within the query fusion framework\")\n\n# Use the same query for consistency across all fusion modes\nquery = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n\ntry:\n    # Create query fusion retriever with RRF mode\n    rrf_query_fusion = QueryFusionRetriever(\n        [base_retriever],\n        similarity_top_k=3,\n        num_queries=3,\n        mode=\"reciprocal_rerank\",\n        use_async=False,\n        verbose=True\n    )\n    \n    print(f\"\\nQuery: {query}\")\n    print(\"QueryFusionRetriever will:\")\n    print(\"1. Generate query variations using LLM\")\n    print(\"2. Retrieve results for each variation\")\n    print(\"3. Apply Reciprocal Rank Fusion\")\n    \n    nodes = rrf_query_fusion.retrieve(query)\n    \n    print(f\"\\nRRF Query Fusion Results:\")\n    for i, node in enumerate(nodes, 1):\n        print(f\"{i}. Final RRF Score: {node.score:.4f}\")\n        print(f\"   Text: {node.text[:100]}...\")\n        print()\n    \n    print(\"RRF Benefits in Query Fusion Context:\")\n    print(\"- Automatically handles query variations of different quality\")\n    print(\"- No bias toward queries that return higher raw scores\")\n    print(\"- Stable performance across diverse query formulations\")\n    \nexcept Exception as e:\n    print(f\"QueryFusionRetriever error: {e}\")\n    print(\"Demonstrating RRF concept manually with query variations...\")\n    \n    # Manual demonstration with query variations derived from the main query\n    query_variations = [\n        DEMO_QUERIES[\"comprehensive\"],  # Original query\n        \"machine learning approaches and methods\",\n        \"different ML techniques and algorithms\"\n    ]\n    \n    print(\"Manual RRF with Query Variations:\")\n    all_results = {}\n    \n    for i, query_var in enumerate(query_variations):\n        print(f\"\\nQuery variation {i+1}: {query_var}\")\n        nodes = base_retriever.retrieve(query_var)\n        \n        # Apply RRF scoring\n        for rank, node in enumerate(nodes):\n            node_id = node.node.node_id\n            if node_id not in all_results:\n                all_results[node_id] = {\n                    'node': node,\n                    'rrf_score': 0,\n                    'query_ranks': []\n                }\n            \n            # Calculate RRF contribution: 1 / (rank + k)\n            k = 60  # Standard RRF parameter\n            rrf_contribution = 1.0 / (rank + 1 + k)\n            all_results[node_id]['rrf_score'] += rrf_contribution\n            all_results[node_id]['query_ranks'].append((i, rank + 1))\n    \n    # Sort by final RRF score\n    sorted_results = sorted(\n        all_results.values(), \n        key=lambda x: x['rrf_score'], \n        reverse=True\n    )\n    \n    print(f\"\\nCombined RRF Results (top 3):\")\n    for i, result in enumerate(sorted_results[:3], 1):\n        print(f\"{i}. Final RRF Score: {result['rrf_score']:.4f}\")\n        print(f\"   Query ranks: {result['query_ranks']}\")\n        print(f\"   Text: {result['node'].text[:100]}...\")\n        print()\n    \n    print(\"RRF Formula Demonstration:\")\n    print(\"For each document: RRF_score = Σ(1 / (rank + 60))\")\n    print(\"- Rank 1 in query: 1/(1+60) = 0.0164\")\n    print(\"- Rank 2 in query: 1/(2+60) = 0.0161\")\n    print(\"- Rank 3 in query: 1/(3+60) = 0.0159\")\n    print(\"Documents appearing in multiple queries get higher combined scores\")"]},{"cell_type":"markdown","id":"5763e01d-ed5e-47b5-bc7c-0fb35d03847b","metadata":{},"outputs":[],"source":["### 6.2 Relative Score Fusion Mode\n","\n","Relative Score Fusion normalizes retrieval scores relative to the maximum score within each query variation's results, enabling effective combination when you want to preserve score magnitude information across different query formulations.\n","\n","**How it works within QueryFusionRetriever**:\n","- Generates multiple query variations using LLM\n","- Retrieves results for each query variation\n","- Normalizes each query's scores by dividing by the maximum score in that query's results\n","- Creates scores in the range [0, 1] where 1 is the best result from each query variation\n","- Combines normalized scores using weighted average or sum\n","\n","**Mathematical approach**:\n","```\n","normalized_score_i(d) = score_i(d) / max_score_i\n","combined_score(d) = Σ (weight_i × normalized_score_i(d))\n","```\n","\n","**Why Relative Score Fusion is valuable for query variations**:\n","- **Preserves score magnitudes**: Unlike RRF, retains information about how confident each query was about its results\n","- **Fair combination**: Ensures no single query variation dominates due to different scoring scales\n","- **Interpretable results**: Final scores reflect the relative strength across query variations\n","- **Flexible weighting**: Can weight certain query formulations more heavily if desired\n","\n","**When to use Relative Score mode**:\n","- When you trust the embedding model's confidence scores\n","- For queries where score magnitudes are meaningful\n","- When different query variations should contribute proportionally to their confidence\n","- In scenarios where you want to understand why certain results ranked highly\n","\n","**Configuration within QueryFusionRetriever**:\n","- Automatically handles score normalization across query variations\n","- Equal weighting of all query variations by default\n","- Preserves relative differences in retriever confidence\n","\n","**Advantages**:\n","- Preserves valuable score magnitude information\n","- Intuitive normalization approach\n","- Works well when retriever scores are reliable\n","- More interpretable than pure rank-based methods\n","\n","**Limitations**:\n","- Sensitive to outlier scores within individual query results\n","- Assumes retriever scores are meaningful and comparable\n","- May not handle unreliable scoring mechanisms well\n","\n","*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/relative_score_dist_fusion/*\n"]},{"cell_type":"code","id":"1dbc07d9-b2d1-46f2-9f16-b5811517bb92","metadata":{},"outputs":[],"source":["print(\"=\" * 60)\nprint(\"6.2 RELATIVE SCORE FUSION MODE DEMONSTRATION\")\nprint(\"=\" * 60)\n\nbase_retriever = lab.vector_index.as_retriever(similarity_top_k=5)\n\nprint(\"Testing QueryFusionRetriever with relative_score mode:\")\nprint(\"This mode preserves score magnitudes while normalizing across query variations\")\n\n# Use the same query for consistency across all fusion modes\nquery = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n\ntry:\n    # Create query fusion retriever with relative score mode\n    rel_score_fusion = QueryFusionRetriever(\n        [base_retriever],\n        similarity_top_k=3,\n        num_queries=3,\n        mode=\"relative_score\",\n        use_async=False,\n        verbose=False\n    )\n    \n    print(f\"\\nQuery: {query}\")\n    print(\"QueryFusionRetriever with relative_score will:\")\n    print(\"1. Generate query variations\")\n    print(\"2. Normalize scores within each variation (score/max_score)\")\n    print(\"3. Combine normalized scores\")\n    \n    nodes = rel_score_fusion.retrieve(query)\n    \n    print(f\"\\nRelative Score Fusion Results:\")\n    for i, node in enumerate(nodes, 1):\n        print(f\"{i}. Combined Relative Score: {node.score:.4f}\")\n        print(f\"   Text: {node.text[:100]}...\")\n        print()\n    \n    print(\"Relative Score Benefits in Query Fusion:\")\n    print(\"- Preserves confidence information from embedding model\")\n    print(\"- Ensures fair contribution from each query variation\")\n    print(\"- More interpretable than rank-only methods\")\n    \nexcept Exception as e:\n    print(f\"QueryFusionRetriever error: {e}\")\n    print(\"Demonstrating Relative Score concept manually...\")\n    \n    # Manual demonstration with query variations derived from the main query\n    query_variations = [\n        DEMO_QUERIES[\"comprehensive\"],  # Original query\n        \"machine learning approaches and methods\",\n        \"different ML techniques and algorithms\"\n    ]\n    \n    print(\"Manual Relative Score Fusion with Query Variations:\")\n    all_results = {}\n    query_max_scores = []\n    \n    # Step 1: Get results and find max scores for each query\n    for i, query_var in enumerate(query_variations):\n        print(f\"\\nQuery variation {i+1}: {query_var}\")\n        nodes = base_retriever.retrieve(query_var)\n        scores = [node.score or 0 for node in nodes]\n        max_score = max(scores) if scores else 1.0\n        query_max_scores.append(max_score)\n        \n        print(f\"Max score for this query: {max_score:.4f}\")\n        \n        # Store results with normalization info\n        for node in nodes:\n            node_id = node.node.node_id\n            original_score = node.score or 0\n            normalized_score = original_score / max_score if max_score > 0 else 0\n            \n            if node_id not in all_results:\n                all_results[node_id] = {\n                    'node': node,\n                    'combined_score': 0,\n                    'contributions': []\n                }\n            \n            all_results[node_id]['combined_score'] += normalized_score\n            all_results[node_id]['contributions'].append({\n                'query': i,\n                'original': original_score,\n                'normalized': normalized_score\n            })\n    \n    # Step 2: Sort by combined relative score\n    sorted_results = sorted(\n        all_results.values(),\n        key=lambda x: x['combined_score'],\n        reverse=True\n    )\n    \n    print(f\"\\nCombined Relative Score Results (top 3):\")\n    for i, result in enumerate(sorted_results[:3], 1):\n        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n        print(f\"   Score breakdown:\")\n        for contrib in result['contributions']:\n            print(f\"     Query {contrib['query']}: {contrib['original']:.3f} → {contrib['normalized']:.3f}\")\n        print(f\"   Text: {result['node'].text[:100]}...\")\n        print()\n    \n    print(\"Relative Score Normalization Process:\")\n    print(\"1. For each query variation, find max_score\")\n    print(\"2. Normalize: normalized_score = original_score / max_score\")\n    print(\"3. Sum normalized scores across all query variations\")\n    print(\"4. Documents with consistently high scores across queries win\")"]},{"cell_type":"markdown","id":"da4737db-11bc-4024-9674-831c706c1adc","metadata":{},"outputs":[],"source":["### 6.3 Distribution-Based Score Fusion Mode\n","\n","Distribution-Based Score Fusion uses statistical properties of score distributions from each query variation to normalize and combine retrieval results, providing the most sophisticated handling of score variability and reliability across different query formulations.\n","\n","**How it works within QueryFusionRetriever**:\n","- Generates multiple query variations using LLM\n","- Analyzes the statistical distribution of scores from each query variation\n","- Normalizes scores using distribution parameters (mean, standard deviation, percentiles)\n","- Applies statistical transformations like z-score normalization or percentile ranking\n","- Combines normalized scores with confidence weighting based on distribution characteristics\n","\n","**Statistical approaches used**:\n","1. **Z-score normalization**: Centers scores around mean with unit variance\n","   - Formula: `z_score = (score - mean) / std_dev`\n","   - Converts to [0,1] range using sigmoid: `1 / (1 + exp(-z_score))`\n","\n","2. **Percentile ranking**: Converts scores to percentile positions\n","   - Formula: `percentile = rank(score) / total_results`\n","\n","3. **Distribution-aware normalization**: Considers score distribution shape\n","   - Uses IQR (Interquartile Range) to adjust for distribution spread\n","   - Handles multi-modal distributions from different query variations\n","\n","**Why Distribution-Based Fusion excels for query variations**:\n","- **Statistical robustness**: Accounts for how scores are distributed within each query variation\n","- **Adaptive weighting**: Can weight query variations based on their score distribution confidence\n","- **Outlier handling**: Statistical methods naturally handle extreme scores\n","- **Multi-modal support**: Each query variation may have different score distribution characteristics\n","\n","**When to use Distribution-Based mode**:\n","- When query variations produce very different score distributions\n","- For complex queries where some variations are much more reliable than others\n","- When you need statistically principled score combination\n","- In scenarios with noisy or unreliable retrieval scoring\n","\n","**Advanced features in QueryFusionRetriever context**:\n","- Automatic distribution analysis for each query variation\n","- Confidence-based weighting of query variations\n","- Robust handling of varying result set sizes\n","- Statistical outlier detection within query results\n","\n","**Advantages**:\n","- Most statistically principled approach to query fusion\n","- Handles complex score distributions effectively\n","- Adapts to different query variation characteristics\n","- Robust to various types of score variability and noise\n","\n","**Limitations**:\n","- Most computationally intensive fusion method\n","- Requires sufficient results for reliable distribution estimation\n","- May over-normalize in some simple scenarios\n","- More complex to interpret than simpler fusion methods\n","\n","*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/relative_score_dist_fusion/*\n"]},{"cell_type":"code","id":"8a738aaa-d3e2-48e6-9ed5-e0953a1b488c","metadata":{},"outputs":[],"source":["print(\"=\" * 60)\nprint(\"6.3 DISTRIBUTION-BASED SCORE FUSION MODE DEMONSTRATION\")\nprint(\"=\" * 60)\n\nbase_retriever = lab.vector_index.as_retriever(similarity_top_k=8)\n\nprint(\"Testing QueryFusionRetriever with dist_based_score mode:\")\nprint(\"This mode uses statistical analysis for the most sophisticated score fusion\")\n\n# Use the same query for consistency across all fusion modes\nquery = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n\ntry:\n    # Create query fusion retriever with distribution-based mode\n    dist_fusion = QueryFusionRetriever(\n        [base_retriever],\n        similarity_top_k=3,\n        num_queries=3,\n        mode=\"dist_based_score\",\n        use_async=False,\n        verbose=False\n    )\n    \n    print(f\"\\nQuery: {query}\")\n    print(\"QueryFusionRetriever with dist_based_score will:\")\n    print(\"1. Generate query variations\")\n    print(\"2. Analyze score distributions for each variation\")\n    print(\"3. Apply statistical normalization (z-score, percentiles)\")\n    print(\"4. Combine with distribution-aware weighting\")\n    \n    nodes = dist_fusion.retrieve(query)\n    \n    print(f\"\\nDistribution-Based Fusion Results:\")\n    for i, node in enumerate(nodes, 1):\n        print(f\"{i}. Statistically Normalized Score: {node.score:.4f}\")\n        print(f\"   Text: {node.text[:100]}...\")\n        print()\n    \n    print(\"Distribution-Based Benefits in Query Fusion:\")\n    print(\"- Accounts for score distribution differences between query variations\")\n    print(\"- Statistically robust against outliers and noise\")\n    print(\"- Adapts weighting based on query variation reliability\")\n    \nexcept Exception as e:\n    print(f\"QueryFusionRetriever error: {e}\")\n    print(\"Demonstrating Distribution-Based concept manually...\")\n    \n    if not SCIPY_AVAILABLE:\n        print(\"⚠️ Full statistical analysis requires scipy\")\n    \n    # Manual demonstration with query variations derived from the main query\n    query_variations = [\n        DEMO_QUERIES[\"comprehensive\"],  # Original query\n        \"machine learning approaches and methods\",\n        \"different ML techniques and algorithms\"\n    ]\n    \n    print(\"Manual Distribution-Based Fusion with Query Variations:\")\n    all_results = {}\n    variation_stats = []\n    \n    # Step 1: Collect results and analyze distributions\n    for i, query_var in enumerate(query_variations):\n        print(f\"\\nQuery variation {i+1}: {query_var}\")\n        nodes = base_retriever.retrieve(query_var)\n        scores = [node.score or 0 for node in nodes]\n        \n        # Calculate distribution statistics\n        mean_score = np.mean(scores) if scores else 0\n        std_score = np.std(scores) if len(scores) > 1 else 1\n        min_score = np.min(scores) if scores else 0\n        max_score = np.max(scores) if scores else 1\n        \n        stats_info = {\n            'mean': mean_score,\n            'std': std_score,\n            'min': min_score,\n            'max': max_score,\n            'nodes': nodes,\n            'scores': scores\n        }\n        variation_stats.append(stats_info)\n        \n        print(f\"Distribution stats: mean={mean_score:.3f}, std={std_score:.3f}\")\n        print(f\"Score range: [{min_score:.3f}, {max_score:.3f}]\")\n        \n        # Apply z-score normalization\n        for node, score in zip(nodes, scores):\n            node_id = node.node.node_id\n            \n            # Z-score normalization\n            if std_score > 0:\n                z_score = (score - mean_score) / std_score\n            else:\n                z_score = 0\n            \n            # Convert to [0,1] using sigmoid\n            normalized_score = 1 / (1 + np.exp(-z_score))\n            \n            if node_id not in all_results:\n                all_results[node_id] = {\n                    'node': node,\n                    'combined_score': 0,\n                    'contributions': []\n                }\n            \n            all_results[node_id]['combined_score'] += normalized_score\n            all_results[node_id]['contributions'].append({\n                'query': i,\n                'original': score,\n                'z_score': z_score,\n                'normalized': normalized_score\n            })\n    \n    # Step 2: Sort by combined distribution-based score\n    sorted_results = sorted(\n        all_results.values(),\n        key=lambda x: x['combined_score'],\n        reverse=True\n    )\n    \n    print(f\"\\nCombined Distribution-Based Results (top 3):\")\n    for i, result in enumerate(sorted_results[:3], 1):\n        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n        print(f\"   Statistical breakdown:\")\n        for contrib in result['contributions']:\n            print(f\"     Query {contrib['query']}: {contrib['original']:.3f} → \"\n                  f\"z={contrib['z_score']:.2f} → {contrib['normalized']:.3f}\")\n        print(f\"   Text: {result['node'].text[:100]}...\")\n        print()\n    \n    print(\"Distribution-Based Process:\")\n    print(\"1. Calculate mean and std for each query variation\")\n    print(\"2. Z-score normalize: z = (score - mean) / std\")\n    print(\"3. Sigmoid transform: normalized = 1 / (1 + exp(-z))\")\n    print(\"4. Sum normalized scores across variations\")\n    print(\"5. Results reflect statistical significance across all query forms\")\n\n# Show fusion mode comparison summary\nprint(\"\\n\" + \"=\" * 60)\nprint(\"FUSION MODES COMPARISON SUMMARY\")\nprint(\"=\" * 60)\nprint(\"All three modes tested with the same query for direct comparison:\")\nprint(f\"Query: {query}\")\nprint()\nprint(\"Mode Characteristics:\")\nprint(\"• RRF (reciprocal_rerank): Most robust, rank-based, scale-invariant\")\nprint(\"• Relative Score: Preserves confidence, normalizes by max score\")  \nprint(\"• Distribution-Based: Most sophisticated, statistical normalization\")\nprint()\nprint(\"Choose based on your use case:\")\nprint(\"- Production stability → RRF\")\nprint(\"- Score interpretability → Relative Score\")\nprint(\"- Statistical robustness → Distribution-Based\")"]},{"cell_type":"markdown","id":"8f3b5eb9-b7c0-4122-b82c-cd777caabaab","metadata":{},"outputs":[],"source":["## Recommended Retrievers by Use Case\n","\n","Based on the authoritative source and the characteristics of each retriever, here are recommended approaches for different scenarios:\n","\n","**General Q&A Applications:**\n","- **Primary**: Vector Index Retriever for semantic understanding\n","- **Enhancement**: Combine with BM25 Retriever using Query Fusion for hybrid approach\n","- **Benefit**: Combines semantic relevance with keyword matching\n","- **From authoritative source**: \"For general Q&A, use a vector index retriever, potentially combined with a BM25 retriever. This retriever fusion combines semantic relevance with keyword matching.\"\n","\n","**Technical Documentation:**\n","- **Primary**: BM25 Retriever for exact term matching\n","- **Enhancement**: Vector Index Retriever as secondary for contextual flexibility\n","- **Benefit**: Prioritizes exact technical terms while maintaining semantic understanding\n","- **From authoritative source**: \"For technical documents, especially those where exact terms need to be prioritized, consider making BM25 your primary retriever, with the vector index retriever adding contextual flexibility as a secondary retriever.\"\n","\n","**Long Documents:**\n","- **Primary**: Auto Merging Retriever\n","- **Benefit**: Retrieves longer parent versions only if enough shorter child versions are retrieved, preserving context\n","- **From authoritative source**: \"For long documents, the auto merging retriever is a great option, because it will retrieve longer parent versions only if enough shorter child versions are retrieved.\"\n","\n","**Research Papers:**\n","- **Primary**: Recursive Retriever\n","- **Benefit**: Follows citations and references to retrieve relevant content from cited papers\n","- **From authoritative source**: \"For research papers, use the recursive retriever in order to retrieve relevant content from cited papers.\"\n","\n","**Large Document Collections:**\n","- **Primary**: Document Summary Index Retriever for initial filtering\n","- **Enhancement**: Followed by Vector Index Retriever for detailed search within relevant documents\n","- **Benefit**: Narrows down relevant documents first, then performs detailed retrieval\n","- **From authoritative source**: \"For large document sets, consider using the document summary index retriever to narrow down the number of relevant documents, followed by a vector search within the remaining subset to retrieve the most pertinent content.\"\n"]},{"cell_type":"markdown","id":"aed15934-104d-4973-a835-663f18d4a4ff","metadata":{},"outputs":[],"source":["---\n","\n","# Exercises\n","\n","Now that you've learned about advanced retrievers, let's practice implementing them in different scenarios.\n"]},{"cell_type":"markdown","id":"aab764cb-5360-4032-a612-adc7a2f91353","metadata":{},"outputs":[],"source":["## Exercise 1 - Build a Custom Hybrid Retriever\n","\n","Your task is to create a hybrid retriever that combines both vector similarity and BM25 keyword search for improved results.\n","\n","**Requirements:**\n","- Use both Vector Index Retriever and BM25 Retriever\n","- Implement a simple score fusion mechanism which takes a weighted average of normalized scores\n","- Test with different query types (semantic vs keyword-focused)\n","\n","**Important Note**: Node IDs from different retrievers won't match even for the same content, so we need to match by text content instead.\n","\n","```python\n","# TODO: Implement hybrid retriever\n","# Step 1: Create both retrievers\n","vector_retriever = # Your code here\n","bm25_retriever = # Your code here\n","\n","# Step 2: Implement score fusion\n","def hybrid_retrieve(query, top_k=5):\n","    # Your implementation here\n","    pass\n","\n","# Step 3: Test with different queries\n","test_queries = [\n","    \"What is machine learning?\",  # Semantic query\n","    \"neural networks deep learning\",  # Keyword query\n","    \"supervised learning techniques\"  # Mixed query\n","]\n","```\n"]},{"cell_type":"code","id":"0097e950-6f54-4f93-8f54-dab89ac47cdc","metadata":{},"outputs":[],"source":["### Put your solution here ###"]},{"cell_type":"markdown","id":"173d3776-5104-4d6c-b517-befe9dcd12f6","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","# Create both retrievers\n","vector_retriever = lab.vector_index.as_retriever(similarity_top_k=10)\n","try:\n","    bm25_retriever = BM25Retriever.from_defaults(\n","        nodes=lab.nodes, similarity_top_k=10\n","    )\n","except:\n","    # Fallback if BM25 is not available\n","    bm25_retriever = vector_retriever\n","\n","def hybrid_retrieve(query, top_k=5):\n","    # Get results from both retrievers\n","    vector_results = vector_retriever.retrieve(query)\n","    bm25_results = bm25_retriever.retrieve(query)\n","    \n","    # Create dictionaries using text content as keys (since node IDs differ)\n","    vector_scores = {}\n","    bm25_scores = {}\n","    all_nodes = {}\n","    \n","    # Normalize vector scores\n","    max_vector_score = max([r.score for r in vector_results]) if vector_results else 1\n","    for result in vector_results:\n","        text_key = result.text.strip()  # Use text content as key\n","        normalized_score = result.score / max_vector_score\n","        vector_scores[text_key] = normalized_score\n","        all_nodes[text_key] = result\n","    \n","    # Normalize BM25 scores\n","    max_bm25_score = max([r.score for r in bm25_results]) if bm25_results else 1\n","    for result in bm25_results:\n","        text_key = result.text.strip()  # Use text content as key\n","        normalized_score = result.score / max_bm25_score\n","        bm25_scores[text_key] = normalized_score\n","        all_nodes[text_key] = result\n","    \n","    # Calculate hybrid scores\n","    hybrid_results = []\n","    for text_key in all_nodes:\n","        vector_score = vector_scores.get(text_key, 0)\n","        bm25_score = bm25_scores.get(text_key, 0)\n","        hybrid_score = 0.7 * vector_score + 0.3 * bm25_score\n","        \n","        hybrid_results.append({\n","            'node': all_nodes[text_key],\n","            'vector_score': vector_score,\n","            'bm25_score': bm25_score,\n","            'hybrid_score': hybrid_score\n","        })\n","    \n","    # Sort by hybrid score and return top k\n","    hybrid_results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n","    return hybrid_results[:top_k]\n","\n","# Test with different queries\n","test_queries = [\n","    \"What is machine learning?\",\n","    \"neural networks deep learning\", \n","    \"supervised learning techniques\"\n","]\n","\n","for query in test_queries:\n","    print(f\"Query: {query}\")\n","    results = hybrid_retrieve(query, top_k=3)\n","    for i, result in enumerate(results, 1):\n","        print(f\"{i}. Hybrid Score: {result['hybrid_score']:.3f}\")\n","        print(f\"   Vector: {result['vector_score']:.3f}, BM25: {result['bm25_score']:.3f}\")\n","        print(f\"   Text: {result['node'].text[:80]}...\")\n","    print()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"7b3b5644-6ec0-4c63-96e6-434de5a8388f","metadata":{},"outputs":[],"source":["## Exercise 2 - Create a Production RAG Pipeline\n","\n","Build a complete RAG pipeline that uses multiple retrieval strategies and includes evaluation metrics.\n","\n","**Requirements:**\n","- Implement retrieval with multiple strategies\n","- Add query routing logic\n","- Include basic evaluation metrics that evaluate whether the pipeline succeeded or failed\n","- Handle edge cases and errors\n","\n","```python\n","# TODO: Implement production RAG pipeline\n","class ProductionRAGPipeline:\n","    def __init__(self, index, llm):\n","        self.index = index\n","        self.llm = llm\n","        # Your initialization code here\n","    \n","    def query(self, question, strategy=\"auto\"):\n","        # Your implementation here\n","        pass\n","    \n","    def evaluate(self, test_queries, expected_answers):\n","        # Your evaluation implementation here\n","        pass\n","\n","# Test the pipeline\n","pipeline = ProductionRAGPipeline(lab.vector_index, llm)\n","```\n"]},{"cell_type":"code","id":"d47468d4-89e2-493d-aa91-a5e5717ec16b","metadata":{},"outputs":[],"source":["### Put your solution here ###"]},{"cell_type":"markdown","id":"08692cdd-2a9a-4996-a746-6832fa3ad60d","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","class ProductionRAGPipeline:\n","    def __init__(self, index, llm):\n","        self.index = index\n","        self.llm = llm\n","        self.vector_retriever = index.as_retriever(similarity_top_k=5)\n","        \n","    def _route_query(self, question):\n","        \"\"\"Simple query routing based on question characteristics\"\"\"\n","        if any(word in question.lower() for word in [\"what\", \"explain\", \"describe\"]):\n","            return \"semantic\"\n","        elif any(word in question.lower() for word in [\"list\", \"types\", \"examples\"]):\n","            return \"comprehensive\"\n","        else:\n","            return \"semantic\"\n","    \n","    def query(self, question, strategy=\"auto\"):\n","        try:\n","            # Route query if strategy is auto\n","            if strategy == \"auto\":\n","                strategy = self._route_query(question)\n","            \n","            # Retrieve relevant documents\n","            if strategy == \"semantic\":\n","                retriever = self.vector_retriever\n","                top_k = 3\n","            elif strategy == \"comprehensive\":\n","                retriever = self.vector_retriever\n","                top_k = 5\n","            else:\n","                retriever = self.vector_retriever\n","                top_k = 3\n","            \n","            # Get relevant documents\n","            relevant_docs = retriever.retrieve(question)\n","            \n","            # Prepare context\n","            context = \"\\n\\n\".join([doc.text for doc in relevant_docs[:top_k]])\n","            \n","            # Generate response\n","            prompt = f\"\"\"Based on the following context, please answer the question:\n","\n","Context:\n","{context}\n","\n","Question: {question}\n","\n","Answer:\"\"\"\n","            \n","            try:\n","                response = self.llm.complete(prompt)\n","                return {\n","                    \"answer\": response.text,\n","                    \"strategy\": strategy,\n","                    \"num_docs\": len(relevant_docs),\n","                    \"status\": \"success\"\n","                }\n","            except Exception as e:\n","                return {\n","                    \"answer\": f\"Based on the retrieved documents: {context[:200]}...\",\n","                    \"strategy\": strategy,\n","                    \"num_docs\": len(relevant_docs),\n","                    \"status\": f\"llm_error: {str(e)}\"\n","                }\n","                \n","        except Exception as e:\n","            return {\n","                \"answer\": \"I encountered an error processing your question.\",\n","                \"strategy\": strategy,\n","                \"num_docs\": 0,\n","                \"status\": f\"error: {str(e)}\"\n","            }\n","    \n","    def evaluate(self, test_queries):\n","        results = []\n","        for query in test_queries:\n","            result = self.query(query)\n","            results.append({\n","                \"query\": query,\n","                \"result\": result,\n","                \"success\": result[\"status\"] == \"success\"\n","            })\n","        \n","        success_rate = sum(1 for r in results if r[\"success\"]) / len(results)\n","        return {\n","            \"success_rate\": success_rate,\n","            \"results\": results\n","        }\n","\n","# Test the pipeline\n","pipeline = ProductionRAGPipeline(lab.vector_index, llm)\n","\n","test_queries = [\n","    \"What is machine learning?\",\n","    \"List different types of learning algorithms\",\n","    \"Explain neural networks\"\n","]\n","\n","print(\"Testing Production RAG Pipeline:\")\n","for query in test_queries:\n","    result = pipeline.query(query)\n","    print(f\"\\nQuery: {query}\")\n","    print(f\"Strategy: {result['strategy']}\")\n","    print(f\"Status: {result['status']}\")\n","    print(f\"Answer: {result['answer'][:100]}...\")\n","\n","# Evaluate performance\n","evaluation = pipeline.evaluate(test_queries)\n","print(f\"\\nPipeline Success Rate: {evaluation['success_rate']:.2%}\")\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"43246f27-0223-425b-9622-389c5878a11c","metadata":{},"outputs":[],"source":["## Summary\n","\n","Congratulations! You've successfully learned about advanced retrievers in LlamaIndex and implemented several practical examples. Here's what you've accomplished:\n","\n","**Key Concepts Mastered:**\n","- **Vector Index Retriever**: Semantic search using embeddings\n","- **BM25 Retriever**: Advanced keyword-based search with TF-IDF improvements\n","- **Document Summary Index**: Intelligent document selection using summaries\n","- **Auto Merging Retriever**: Hierarchical context preservation\n","- **Recursive Retriever**: Multi-level reference following\n","- **Query Fusion Retriever**: Multi-query enhancement with three fusion modes\n","\n","**Practical Skills Developed:**\n","- Implementing hybrid retrieval strategies\n","- Combining different retrieval methods effectively\n","- Building production-ready RAG pipelines\n","- Evaluating retrieval performance\n","\n","**Best Practices Learned:**\n","- When to use each retrieval method\n","- How to combine multiple retrieval strategies\n","- Production considerations for RAG systems\n","- Evaluation techniques for retrieval quality\n","\n","**Next Steps:**\n","- Experiment with different embedding models\n","- Implement more sophisticated fusion techniques\n","- Add reranking models for improved precision\n","- Scale to larger document collections\n","- Integrate with production systems\n","\n","---\n"]},{"cell_type":"markdown","id":"c52c3ce9-cb99-463e-a4bd-bc1f8e29da60","metadata":{},"outputs":[],"source":["## Authors\n","\n","[Wojciech \\\"Victor\\\" Fulmyk](https://www.linkedin.com/in/wfulmyk)\n","\n","Wojciech \"Victor\" Fulmyk is a Data Scientist at IBM\n","\n","<!--## Change Log\n","\n","<details>\n","    <summary>Click here for the changelog</summary>\n","\n","|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2025-07-18|0.1|Wojciech \"Victor\" Fulmyk|Initial version|\n","\n","</details>\n","-->\n","\n","---\n","\n","Copyright © IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"prev_pub_hash":"38fbe24a85cef87de7f7211df2f38f951e6904d22e34f03df005ed3dfb74baf6"},"nbformat":4,"nbformat_minor":4}